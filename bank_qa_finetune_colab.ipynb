{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dy5aa2ml2xl"
      },
      "source": [
        "# Fine-tuning Mistral-7B con QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wQtTSXPl2xn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ScB2GynzpI-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/xaviguardia/llm01.git && cd llm01"
      ],
      "metadata": {
        "id": "oqQEN8F_pJUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "# set the directory to /content/llm01\n",
        "import os\n",
        "os.chdir(\"/content/llm01\")\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "dataset_path = \"data/qa_dataset.jsonl\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n"
      ],
      "metadata": {
        "id": "wStyhbD5mkSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(token=hf_token)\n",
        "\n",
        "# Load the JSON data using pandas\n",
        "df = pd.read_json(dataset_path, lines=True)\n",
        "\n",
        "# Convert the pandas DataFrame to a datasets Dataset object\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "def tokenize(sample):\n",
        "    tokenized_sample = tokenizer(\n",
        "        sample[\"prompt\"] + \"\\n\" + sample[\"response\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    tokenized_sample[\"labels\"] = tokenized_sample[\"input_ids\"].copy()\n",
        "    return tokenized_sample\n",
        "\n",
        "tokenized = dataset.map(tokenize)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./outputs\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    fp16=True, # Descomenta si tu GPU lo soporta\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=args, train_dataset=tokenized)\n",
        "trainer.train()\n",
        "# Save the trained model\n",
        "trainer.save_model(\"./fine_tuned_model\")"
      ],
      "metadata": {
        "id": "s73Jv_cHsnOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# el objetivo de la celda es cargar el modelo y el finetuneado\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "fine_tuned_model_path = \"./fine_tuned_model\"\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"gpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Try the trained model\n",
        "# Load the fine-tuned model\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Set the directory to /content/llm01 if it's not already\n",
        "if os.getcwd() != '/content/llm01':\n",
        "    os.chdir('/content/llm01')\n",
        "\n",
        "\n",
        "fine_tuned_model_path = \"./fine_tuned_model\"\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"gpu\")\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Load the PEFT model (apply the LoRA adapters) onto the base model\n",
        "model = PeftModel.from_pretrained(base_model, fine_tuned_model_path)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "print(\"Fine-tuned model loaded and pipeline created successfully!\")"
      ],
      "metadata": {
        "id": "ah6ppEAH0evz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example prompt\n",
        "prompt = \"¿Qué hace el interceptor de seguridad en las peticiones HTTP?\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "print(generated_text[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "vzuze7fu2qZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}